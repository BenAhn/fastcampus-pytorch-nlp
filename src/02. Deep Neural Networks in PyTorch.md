<!-- $size: 16:9 -->
<link rel='stylesheet' href='slides.marp.css'>

# Natural Language Processing<br>with PyTorch

**Week 2** Deep Neural Networks in PyTorch

---

## Review & Warranty

* PyTorch installation
* Activation functions, ReLU's problems
* Loss functions: objective function != loss function
* Conda commands: how source activate works
* Jupyter kernel
* Docker commands
* cross-validation or n-fold https://www.quora.com/Is-cross-validation-heavily-used-in-deep-learning-or-is-it-too-expensive-to-be-used

---

* Overview of PyTorch modules
* Product deployment
  * Research in PyTorch, deploy using [Open Neural Network Exchange (ONNX)](https://onnx.ai/).

---

* Typical project workflow
  * Obtain data
  * Overfit using latest baseline models
  * Tune and optimize
    * Hard example mining, model pruning, custom loss functions, etc.

audience & goal: not something on the web
- environment setup
- working examples and base code to start
- learning map
- support
microphone..
supplement sessions
python bootcamp

---

### Additional materials

#### Beginner

This course supposes basic knowledge in Python and mathematics (statistics, probability, linear algebra, and calculus). If you are unfamiliar with those topics, consider the first two materials as required for following hands-on sessions, and the third for a detailed understanding of the basic theory.

* [무작정 따라하는 파이썬 프로그래밍 (YouTube)](https://www.youtube.com/watch?v=ub9Dh9IwSrk)
* [PyTorch Zero to All by Sung Kim (YouTube)](https://www.youtube.com/playlist?list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m)
  * 우리말이 더 편하시다면, TensorFlow 기준이지만 [모두를 위한 딥러닝/머신러닝 by Sung Kim (YouTube)](https://www.youtube.com/playlist?list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm)
* [Machine Learning by Andrew Ng (Coursera)](https://www.coursera.org/learn/machine-learning)

---

#### Intermediate

The materials below cover the theoretical parts of this course. They are
recommended for further commitment.

* [Deep Learning Book by Ian Goodfellow et al.](https://www.deeplearningbook.org/)
  * 딥러닝에 필요한 기초 선형대수 및 미적분부터 차근차근 설명합니다.
* [CS231n: Convolutional Neural Networks for Visual Recognition at Standford](https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk)
  * [Materials](http://cs231n.stanford.edu/index.html)
* [CS224d: Deep Learning for Natural Language Processing at Standford](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)
  * [Materials](http://cs224d.stanford.edu/index.html)
  * Or, [Oxford Deep NLP 2017](https://github.com/oxford-cs-deepnlp-2017/lectures)
* [Google 머신러닝 단기집중과정](https://developers.google.com/machine-learning/crash-course/?hl=ko)

---

#### Advanced

* [Pattern Recognition and Machine Learning 우리말 정리](http://norman3.github.io/prml/)

#### Other supplementary materials

* [라온피플 머신러닝 아카데미](https://laonple.blog.me/220463627091)
* [colah's blog](http://colah.github.io/)
  * 잘 찾아보시면 번역본도 몇 편 있습니다.

---

## Contents

1. Convolutional Neural Networks
   * The convolutional layer
   * Batch normalization
   * Residual connections
   * Variations: dilated convolution, deconvolution, separable convolution

---

## Contents

2. Recurrent Neural Networks
   * The recurrent layer
   * Gradient vanishing and exploding
   * Gradient clipping
   * Variations: Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU)

---

## Contents

3. Cryptocurrency price prediction using CNN and RNN
   * Obtaining and preprocessing the data
   * Building our first CNN model
   * Building our first RNN model
   * Running live

---

<!-- *template: section -->

## Convolutional Neural Networks

---

### Contents

* The convolutional layer
* Batch normalization
* Residual connections
* Variations: dilated convolution, deconvolution, separable convolution

---

### The convolutional layer

https://github.com/vdumoulin/conv_arithmetic

---

### Batch normalization

Batch NormalizationL Accelerating Deep Network Training by Reducing Internal Covariate Shift

https://arxiv.org/abs/1502.03167

https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/

---

### Residual connections

Deep Residual Learning for Image Recognition

https://arxiv.org/abs/1512.03385

Densely Connected Convolutional Networks

https://arxiv.org/abs/1608.06993

CondenseNet: An Efficient DenseNet using Learned Group Convolutions

https://arxiv.org/abs/1711.09224

---

### Variations

#### Dilated convolution

a.k.a. atrous convolution

---

### Variations

#### Deconvolution

a.k.a. transposed convolution, fractionally strided convolution, upconvolution

https://github.com/vdumoulin/conv_arithmetic

---

### Variations

#### Separable convolution

http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/

MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications

https://arxiv.org/abs/1704.04861

---

<!-- *template: section -->

## Recurrent <br>Neural Networks

---

### Contents

* The recurrent layer
* Gradient vanishing and exploding
* Gradient clipping
* Variations: Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU)

---

### The recurrent layer

http://colah.github.io/posts/2015-08-Understanding-LSTMs/

---

### Gradient vanishing and exploding

---

### Gradient clipping

---

### Variations

#### Long Short-Term Memory (LSTM)

---

### Variations

#### Gated Recurrent Unit (GRU)

Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (2014)

https://arxiv.org/abs/1406.1078

---

<!-- *template: section -->

## Cryptocurrency price prediction

---

### Contents

* Obtaining and preprocessing the data
* Building our first CNN model
* Building our first RNN model
* Running live

---

### Obtaining and preprocessing the data

---

### Building our first CNN model

---

### Building our first RNN model

---

### Running live

---

<!-- *template: center -->

## Thank you!
