## Quick preparation

1. Install Anaconda.
   * https://conda.io/docs/user-guide/install/index.html

2. Open Anaconda console, and create a new virtual environment called `pytorch-nlp`.
   * `conda create -y --name pytorch-nlp python=3.6 numpy pyyaml scipy ipython mkl tqdm`

3. Install PyTorch on `pytorch-nlp` (this may take a while).
  * `conda install --name pytorch-nlp pytorch-cpu torchvision -c pytorch`

---

<!--
  *template: center
  *footer: A knowledge base system
-->

![bg]("images/01/knowledge_base.jpg")

---

<!--
  *template: center
  *footer: A modelling of the McCulloch-Pitts neuron
-->

![bg]("images/01/mcculloch_pitts_neuron.jpg")

---

## Neural Networks

* Feed-forward networks

```python
def perceptron(inputs, weights, biases):
    return sum(inputs * weights + biases)
```

---

<!--
  *template: center
  *footer: http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/
-->

Problem: draw a single straight line to separate colors.

![bg]("images/01/linear_classifier_1.jpg")

---

<!--
  *template: center
  *footer: http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/
-->

Problem: draw a single straight line to separate colors.

![bg]("images/01/linear_classifier_2.jpg")

---

![bg]("images/01/matrix.jpg")

---

<!--
  *template: center
  *footer: http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/
-->

Problem: draw a single straight line to separate colors.

![bg]("images/01/linear_classifier_3.jpg")

---

<!--
  *template: center
  *footer: http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/
-->

Problem: draw a single straight line to separate colors.

![bg]("images/01/linear_classifier_4.jpg")

---

```python
def backpropagate(weights, derivative, learning_rate):
    return weights - learning_rate * (derivative - weights)
```

---

```python
def sigmoid(inputs):
    return 1.0 / (1.0 + exp(-inputs))
```

---

```python
def relu(inputs):
    return max(0, inputs)
```

---

<!-- *footer: http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture03.pdf
-->

![bg](images/01/classification_results.png)

---

```python
def softmax(inputs):
    return exp(inputs) / sum(exp(inputs))
```

---

* L1 distance (Manhattan distance)

---

* L2 distance (Euclidean distance)

---

```python
def l1_loss(targets, outputs):
    return sum(abs(targets - outputs))
```

---

```python
def l2_loss(targets, outputs):
    return sum(sqrt((targets - outputs)**2))
```

---

```python
def mean_square_error(targets, outputs):
    return mean(sqrt((targets - outputs)**2))
```

---

Information theory

entropy = amount of surprise

![](images/01/log_function.jpg)

$$h[x] = -log p(x)$$

---

$$H(X, Y) = \sum p(x) \times log(q(y))$$

![](cross_entropy.jpg)
https://towardsdatascience.com/deep-learning-concepts-part-1-ea0b14b234c8

```python
def cross_entropy_loss(targets, outputs):
    return -sum(targets * log(outputs))
```

---

```python
def binary_cross_entropy_loss(targets, outputs):
    return -mean(targets * log(outputs) + (1 - targets) * log(1 - outputs))
```

---

```python
def backpropagate(weights, derivative, learning_rate, weight_decay):
    weight_penalty = weight_decay * sum(sqrt(weights ** 2))
    return weights - learning_rate * (derivative @ weights + weight_penalty)
```

---

fashion mnist

resnet

file structure

`git clone` or download https://github.com/juneoh/sample_pytorch_project

* `.gitignore`
* `Dockerfile`
* `main.py`
* `README.md`
* `requirements.txt`


1. Prepare data.
2. Prepare models.

--

`torch.nn.Module`
`torch.utils.data.DataLoader`
