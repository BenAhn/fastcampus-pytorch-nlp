{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (2013)\n",
    "\n",
    "[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "by Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean\n",
    "\n",
    "![](http://i.imgur.com/agTBWiT.png)\n",
    "\n",
    "### Continuous Bag-Of-Words vs. Skip-gram\n",
    "\n",
    "* CBOW: guessing the blank\n",
    "* Skip-gram: guessing the neighbors\n",
    "\n",
    "![](https://ascelibrary.org/cms/attachment/83d45b70-be2d-4dae-a37a-e3b51af0b7c4/figure3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NANO_CORPUS = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'a', 'computational', 'process', 'computational', 'processes', 'are', 'abstract', 'beings', 'that', 'inhabit', 'computers', 'as', 'they', 'evolve', 'processes', 'manipulate', 'other', 'abstract', 'things', 'called', 'data', 'the', 'evolution', 'of', 'a', 'process', 'is', 'directed', 'by', 'a', 'pattern', 'of', 'rules', 'called', 'a', 'program', 'people', 'create', 'programs', 'to', 'direct', 'processes', 'in', 'effect', 'we', 'conjure', 'the', 'spirits', 'of', 'the', 'computer', 'with', 'our', 'spells']\n"
     ]
    }
   ],
   "source": [
    "corpus = NANO_CORPUS.lower().replace(',', ' ').replace('.', ' ').split()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vocabulary = list(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin anything, we need to create a one-hot vector of the words. Pandas is great at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = torch.FloatTensor(len(vocabulary), EMBEDDING_SIZE).uniform_()\n",
    "        self.linear1 = torch.nn.Linear(EMBEDDING_SIZE, 128)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(128, len(vocabulary))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sum(self.embeddings * x.sum(dim=0).view(-1, 1), dim=0)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x.view(1, -1)\n",
    "    \n",
    "    def get_word_embedding(self, word):\n",
    "        return self.embeddings[vocabulary.index(word)].view(1, -1)\n",
    "\n",
    "cbow = CBOW()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 3.8188\n",
      "Epoch 1: loss 3.7716\n",
      "Epoch 2: loss 3.7352\n",
      "Epoch 3: loss 3.7072\n",
      "Epoch 4: loss 3.6840\n",
      "Epoch 5: loss 3.6644\n",
      "Epoch 6: loss 3.6472\n",
      "Epoch 7: loss 3.6329\n",
      "Epoch 8: loss 3.6191\n",
      "Epoch 9: loss 3.6072\n",
      "Epoch 10: loss 3.5957\n",
      "Epoch 11: loss 3.5846\n",
      "Epoch 12: loss 3.5741\n",
      "Epoch 13: loss 3.5641\n",
      "Epoch 14: loss 3.5537\n",
      "Epoch 15: loss 3.5436\n",
      "Epoch 16: loss 3.5335\n",
      "Epoch 17: loss 3.5236\n",
      "Epoch 18: loss 3.5136\n",
      "Epoch 19: loss 3.5038\n",
      "Epoch 20: loss 3.4943\n",
      "Epoch 21: loss 3.4844\n",
      "Epoch 22: loss 3.4744\n",
      "Epoch 23: loss 3.4649\n",
      "Epoch 24: loss 3.4551\n",
      "Epoch 25: loss 3.4453\n",
      "Epoch 26: loss 3.4347\n",
      "Epoch 27: loss 3.4244\n",
      "Epoch 28: loss 3.4139\n",
      "Epoch 29: loss 3.4038\n",
      "Epoch 30: loss 3.3933\n",
      "Epoch 31: loss 3.3828\n",
      "Epoch 32: loss 3.3721\n",
      "Epoch 33: loss 3.3616\n",
      "Epoch 34: loss 3.3507\n",
      "Epoch 35: loss 3.3397\n",
      "Epoch 36: loss 3.3281\n",
      "Epoch 37: loss 3.3167\n",
      "Epoch 38: loss 3.3053\n",
      "Epoch 39: loss 3.2938\n",
      "Epoch 40: loss 3.2825\n",
      "Epoch 41: loss 3.2704\n",
      "Epoch 42: loss 3.2592\n",
      "Epoch 43: loss 3.2466\n",
      "Epoch 44: loss 3.2354\n",
      "Epoch 45: loss 3.2224\n",
      "Epoch 46: loss 3.2108\n",
      "Epoch 47: loss 3.1984\n",
      "Epoch 48: loss 3.1856\n",
      "Epoch 49: loss 3.1726\n",
      "Epoch 50: loss 3.1601\n",
      "Epoch 51: loss 3.1470\n",
      "Epoch 52: loss 3.1346\n",
      "Epoch 53: loss 3.1211\n",
      "Epoch 54: loss 3.1082\n",
      "Epoch 55: loss 3.0949\n",
      "Epoch 56: loss 3.0814\n",
      "Epoch 57: loss 3.0672\n",
      "Epoch 58: loss 3.0534\n",
      "Epoch 59: loss 3.0398\n",
      "Epoch 60: loss 3.0256\n",
      "Epoch 61: loss 3.0112\n",
      "Epoch 62: loss 2.9966\n",
      "Epoch 63: loss 2.9825\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 64\n",
    "WINDOW_SIZE = 2\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "def get_context(i, corpus):\n",
    "    context = []\n",
    "    \n",
    "    start = max(i - WINDOW_SIZE, 0)\n",
    "    end = min(i + WINDOW_SIZE, len(corpus) - 1)\n",
    "    \n",
    "    for n in range(start, end):\n",
    "        if n == i:\n",
    "            continue\n",
    "        context.append(corpus[n])\n",
    "    \n",
    "    return context\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    n_words = 0\n",
    "    acc_loss = 0\n",
    "    for i, word in enumerate(corpus):\n",
    "        context = torch.FloatTensor(\n",
    "            [one_hot[word] for word in get_context(i, corpus)])\n",
    "        target = torch.LongTensor([vocabulary.index(word)])\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            output = cbow(context)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += float(loss)\n",
    "            n_words += 1\n",
    "\n",
    "    print(f'Epoch {epoch}: loss {acc_loss/n_words:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember our corpus?\n",
    "\n",
    "> We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create **programs** to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\n",
    "\n",
    "Let's see if our model can guess the highlighted word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes\n"
     ]
    }
   ],
   "source": [
    "quiz = ['people', 'create', 'to', 'direct']\n",
    "output = cbow(torch.FloatTensor([one_hot[w] for w in quiz]))\n",
    "_, i = output.max(dim=1)\n",
    "print(vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8489, 0.9017, 0.1659, 0.9903, 0.4600, 0.0077, 0.8274, 0.8722, 0.4643,\n",
       "         0.9406, 0.1229, 0.9057, 0.7660, 0.2854, 0.8600, 0.1224, 0.8084, 0.1031,\n",
       "         0.0346, 0.8578, 0.3230, 0.7069, 0.6619, 0.9268, 0.3734, 0.4252, 0.5809,\n",
       "         0.6659, 0.0659, 0.7780, 0.2868, 0.2424, 0.4232, 0.5825, 0.5407, 0.5262,\n",
       "         0.5431, 0.6958, 0.7654, 0.7655, 0.9760, 0.6818, 0.2394, 0.6063, 0.5122,\n",
       "         0.6355, 0.1387, 0.6136, 0.1484, 0.9226, 0.1001, 0.2557, 0.1510, 0.1334,\n",
       "         0.8858, 0.8536, 0.8248, 0.1578, 0.3673, 0.1430, 0.4194, 0.4097, 0.2029,\n",
       "         0.1457, 0.8110, 0.3857, 0.3127, 0.6986, 0.2857, 0.0669, 0.6471, 0.9752,\n",
       "         0.1169, 0.6310, 0.7778, 0.0909, 0.2148, 0.5374, 0.5005, 0.6790, 0.9398,\n",
       "         0.9529, 0.0839, 0.6369, 0.1430, 0.5669, 0.7131, 0.8976, 0.0221, 0.3495,\n",
       "         0.9691, 0.9676, 0.8904, 0.6848, 0.7213, 0.9607, 0.1187, 0.0618, 0.3771,\n",
       "         0.7712, 0.5259, 0.9896, 0.1445, 0.3457, 0.6096, 0.5555, 0.1200, 0.9508,\n",
       "         0.9294, 0.4935, 0.3309, 0.0815, 0.5252, 0.6848, 0.6766, 0.2585, 0.1180,\n",
       "         0.7250, 0.7095, 0.5985, 0.7497, 0.8628, 0.0241, 0.0382, 0.1659, 0.9823,\n",
       "         0.0369, 0.1810]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.get_word_embedding('programs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embeddings = torch.FloatTensor(len(vocabulary), EMBEDDING_SIZE).normal_()\n",
    "        self.linear1 = torch.nn.Linear(EMBEDDING_SIZE, 128)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(128, len(vocabulary))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings[x]\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x.view(1, -1)\n",
    "    \n",
    "    def get_word_embedding(self, word):\n",
    "        return self.embeddings[vocabulary.index(word)].view(1, -1)\n",
    "\n",
    "skipgram = Skipgram()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(skipgram.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 3.7707\n",
      "Epoch 1: loss 3.4202\n",
      "Epoch 2: loss 3.1437\n",
      "Epoch 3: loss 2.8998\n",
      "Epoch 4: loss 2.6799\n",
      "Epoch 5: loss 2.4936\n",
      "Epoch 6: loss 2.3523\n",
      "Epoch 7: loss 2.2548\n",
      "Epoch 8: loss 2.1937\n",
      "Epoch 9: loss 2.1552\n",
      "Epoch 10: loss 2.1308\n",
      "Epoch 11: loss 2.1155\n",
      "Epoch 12: loss 2.1030\n",
      "Epoch 13: loss 2.0913\n",
      "Epoch 14: loss 2.0818\n",
      "Epoch 15: loss 2.0730\n",
      "Epoch 16: loss 2.0654\n",
      "Epoch 17: loss 2.0574\n",
      "Epoch 18: loss 2.0503\n",
      "Epoch 19: loss 2.0435\n",
      "Epoch 20: loss 2.0366\n",
      "Epoch 21: loss 2.0307\n",
      "Epoch 22: loss 2.0243\n",
      "Epoch 23: loss 2.0168\n",
      "Epoch 24: loss 2.0101\n",
      "Epoch 25: loss 2.0066\n",
      "Epoch 26: loss 1.9998\n",
      "Epoch 27: loss 1.9944\n",
      "Epoch 28: loss 1.9898\n",
      "Epoch 29: loss 1.9845\n",
      "Epoch 30: loss 1.9803\n",
      "Epoch 31: loss 1.9746\n",
      "Epoch 32: loss 1.9709\n",
      "Epoch 33: loss 1.9668\n",
      "Epoch 34: loss 1.9631\n",
      "Epoch 35: loss 1.9589\n",
      "Epoch 36: loss 1.9551\n",
      "Epoch 37: loss 1.9510\n",
      "Epoch 38: loss 1.9477\n",
      "Epoch 39: loss 1.9440\n",
      "Epoch 40: loss 1.9401\n",
      "Epoch 41: loss 1.9391\n",
      "Epoch 42: loss 1.9345\n",
      "Epoch 43: loss 1.9311\n",
      "Epoch 44: loss 1.9285\n",
      "Epoch 45: loss 1.9255\n",
      "Epoch 46: loss 1.9215\n",
      "Epoch 47: loss 1.9192\n",
      "Epoch 48: loss 1.9153\n",
      "Epoch 49: loss 1.9128\n",
      "Epoch 50: loss 1.9103\n",
      "Epoch 51: loss 1.9078\n",
      "Epoch 52: loss 1.9056\n",
      "Epoch 53: loss 1.9040\n",
      "Epoch 54: loss 1.8996\n",
      "Epoch 55: loss 1.8972\n",
      "Epoch 56: loss 1.8948\n",
      "Epoch 57: loss 1.8914\n",
      "Epoch 58: loss 1.8882\n",
      "Epoch 59: loss 1.8864\n",
      "Epoch 60: loss 1.8838\n",
      "Epoch 61: loss 1.8810\n",
      "Epoch 62: loss 1.8796\n",
      "Epoch 63: loss 1.8770\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 64\n",
    "WINDOW_SIZE = 2\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "def get_context(i, corpus):\n",
    "    context = []\n",
    "    \n",
    "    start = max(i - WINDOW_SIZE, 0)\n",
    "    end = min(i + WINDOW_SIZE, len(corpus) - 1)\n",
    "    \n",
    "    for n in range(start, end):\n",
    "        if n == i:\n",
    "            continue\n",
    "        context.append(corpus[n])\n",
    "    \n",
    "    return context\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    n_words = 0\n",
    "    acc_loss = 0\n",
    "    for i, word in enumerate(corpus):\n",
    "        center = vocabulary.index(word)\n",
    "\n",
    "        for word in get_context(i, corpus):\n",
    "            context = torch.LongTensor([vocabulary.index(word)])\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                output = skipgram(center)\n",
    "                loss = criterion(output, context)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                acc_loss += float(loss)\n",
    "                n_words += 1\n",
    "\n",
    "    print(f'Epoch {epoch}: loss {acc_loss/n_words:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people       0.150772\n",
       "programs     0.032084\n",
       "beings       0.029256\n",
       "spirits      0.021663\n",
       "abstract     0.020167\n",
       "process      0.019939\n",
       "direct       0.018144\n",
       "data         0.017356\n",
       "computers    0.014036\n",
       "inhabit      0.013199\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_similar(query, embeddings, top_k=10):\n",
    "    embeddings = embeddings.cpu()\n",
    "    query = embeddings[vocabulary.index(query)]\n",
    "    similarity = (embeddings @ query) / (embeddings.norm() * query.norm())\n",
    "    similarity = pd.Series(dict(zip(vocabulary, similarity.numpy())))\n",
    "    similarity = similarity.sort_values(ascending=False)\n",
    "    \n",
    "    return similarity[:top_k]\n",
    "\n",
    "get_similar('people', skipgram.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe: Global Vectors for Word Representation (2014)\n",
    "\n",
    "by Jeffrey Pennington, Richard Socher, Christopher D. Manning\n",
    "\n",
    "https://www.aclweb.org/anthology/D14-1162\n",
    "\n",
    "On page 1534:\n",
    "\n",
    "> We begin with a simple example that showcases\n",
    "how certain aspects of meaning can be extracted\n",
    "directly from co-occurrence probabilities. Consider\n",
    "two words $i$ and $j$ that exhibit a particular aspect\n",
    "of interest; for concreteness, suppose we are\n",
    "interested in the concept of thermodynamic phase,\n",
    "for which we might take $i = ice$ and $j = steam$.\n",
    "The relationship of these words can be examined\n",
    "by studying the ratio of their co-occurrence probabilities\n",
    "with various probe words, $k$. For words\n",
    "$k$ related to $ice$ but not $steam$, say $k = solid$, we\n",
    "expect the ratio $Pik / Pjk$ will be large. Similarly,\n",
    "for words $k$ related to $steam$ but not $ice$, say $k =\n",
    "gas$, the ratio should be small. For words $k$ like\n",
    "$water$ or $fashion$, that are either related to both $ice$\n",
    "and $steam$, or to neither, the ratio should be close\n",
    "to one. Table 1 shows these probabilities and their\n",
    "ratios for a large corpus, and the numbers confirm\n",
    "these expectations. Compared to the raw probabilities,\n",
    "the ratio is better able to distinguish relevant\n",
    "words ($solid$ and $gas$) from irrelevant words ($water$\n",
    "and $fashion$) and it is also better able to discriminate\n",
    "between the two relevant words.\n",
    "\n",
    "$$\n",
    "\\frac{P_{solid | ice}}{P_{solid | steam}} >\n",
    "\\frac{P_{fashion | ice}}{P_{fashion | steam}} >\n",
    "\\frac{P_{gas | ice}}{P_{gas | steam}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above argument suggests that the appropriate\n",
    "starting point for word vector learning should\n",
    "be with ratios of co-occurrence probabilities rather\n",
    "than the probabilities themselves. Noting that the\n",
    "ratio $P_{ik} /P_{jk}$ depends on three words $i$, $j$, and $k$,\n",
    "the most general model takes the form,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i, w_j, \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The number of possibilities for $F$ is vast,\n",
    "but by enforcing a few desiderata we can select a\n",
    "unique choice. First, we would like $F$ to encode\n",
    "the information present the ratio $Pik / Pjk$ in the\n",
    "word vector space. Since vector spaces are inherently\n",
    "linear structures, the most natural way to do\n",
    "this is with vector differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i - w_j, \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, we note that the arguments of $F$ in Eqn. (2)\n",
    "are vectors while the right-hand side is a scalar.\n",
    "While $F$ could be taken to be a complicated function\n",
    "parameterized by, e.g., a neural network, doing\n",
    "so would obfuscate the linear structure we are\n",
    "trying to capture. To avoid this issue, we can first\n",
    "take the dot product of the arguments,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F((w_i - w_j)^T \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, note that for\n",
    "word-word co-occurrence matrices, the distinction\n",
    "between a word and a context word is arbitrary and\n",
    "that we are free to exchange the two roles. To do so\n",
    "consistently, we must not only exchange $w \\leftrightarrow \\tilde{w}$\n",
    "but also $X \\leftrightarrow X^T$. Our final model should be invariant\n",
    "under this relabeling, but Eqn. (3) is not.\n",
    "However, the symmetry can be restored in two\n",
    "steps. First, we require that $F$ be a homomorphism\n",
    "between the groups $(\\mathbb{R}, +)$ and $(\\mathbb{R}_{>0}, \\times)$, i.e.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(X-Y)=\\frac { F(X) }{ F(Y) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i^T \\tilde{w}_k - w_j^T \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i^T \\tilde{w}_k - w_j^T \\tilde{w}_k) = \\frac{F(w_i^T \\tilde{w}_k)}{F(w_j^T \\tilde{w}_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\exp(w_i^T \\tilde{w}_k - w_j^T \\tilde{w}_k) = \\frac{\\exp(w_i^T \\tilde{w}_k)}{\\exp(w_j^T \\tilde{w}_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F = \\exp$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 1533:\n",
    "> Let the matrix\n",
    "of word-word co-occurrence counts be denoted by\n",
    "$X$, whose entries $X_{ij}$ tabulate the number of times\n",
    "word $j$ occurs in the context of word $i$. Let $X_i = \\sum_k X_{ik}$\n",
    "be the number of times any word appears\n",
    "in the context of word $i$. Finally, let\n",
    "$P_{ij} = P(j|i) = X_{ij}/X_i$be the probability that word $j$ appear in the\n",
    "context of word $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i^T \\tilde{w}_k) = P_{ik} = \\frac{X_{ik}}{X_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_i^T \\tilde { w }_k =\\log { { P }_{ ik } } =\\log ({ X_{ ik })-\\log ({ X_{ i } })  }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 1535:\n",
    "> Next, we note that Eqn. (6) would exhibit the exchange\n",
    "symmetry if not for the $log(X_i)$ on the\n",
    "right-hand side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\log(X_{ik})-\\log(X_i) \\neq \\log(X_{ki})-\\log(X_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, this term is independent\n",
    "of $k$ so it can be absorbed into a bias $b_i$ for\n",
    "$w+i$. Finally, adding an additional bias $\\tilde{b}_k$ for $\\tilde{w}_k$\n",
    "restores the symmetry,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{ w }_{ i }^{ T }\\tilde { { w }_{ k } } +{ b }_{ i }+\\tilde { { b }_{ k } } =\\log ({ X_{ ik }})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the vocabulary and counting co-occurrence (again)\n",
    "\n",
    "Today's dataset, an English monolingual corpus, can be found [here](https://drive.google.com/open?id=1__lK0x_k8gtyV27QZqQUGSC4jlaQAZSC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "FILE = 'ted.en.txt'\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "vocabulary = defaultdict(int)\n",
    "co_occurrence = defaultdict(int)\n",
    "\n",
    "with open(FILE) as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split(' ')\n",
    "    for i in range(len(words)):\n",
    "        vocabulary[words[i]] += 1\n",
    "\n",
    "        for j in range(i + 1, i + WINDOW_SIZE + 1):\n",
    "            if j >= len(words):\n",
    "                break\n",
    "            keys = tuple(sorted([words[i], words[j]]))\n",
    "            co_occurrence[keys] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much words we have gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77599"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some love!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'love' in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2444"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['love']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the dictionary into a Pandas Series for convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MIN_OCCURRENCE = 10\n",
    "\n",
    "vocabulary = pd.Series(vocabulary, dtype='uint16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with the help of Pandas, let's set a minimum frequency threshold to trim the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16754"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = vocabulary[vocabulary >= MIN_OCCURRENCE]\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'love' in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_ij = np.zeros((len(vocabulary), len(vocabulary)), dtype='uint16')\n",
    "\n",
    "for (word_i, word_j), value in co_occurrence.items():\n",
    "    try:\n",
    "        i = vocabulary.index.get_loc(word_i)\n",
    "        j = vocabulary.index.get_loc(word_j)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    X_ij[i][j] = value\n",
    "    X_ij[j][i] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{ w }_{ i }^{ T }\\tilde { { w }_{ k } } +{ b }_{ i }+\\tilde { { b }_{ k } } =\\log ({ X_{ ik }})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 0.2815\n",
      "iteration 1, loss 0.1468\n",
      "iteration 2, loss 0.0932\n",
      "iteration 3, loss 0.0726\n",
      "iteration 4, loss 0.0588\n",
      "iteration 5, loss 0.0514\n",
      "iteration 6, loss 0.0481\n",
      "iteration 7, loss 0.0448\n",
      "iteration 8, loss 0.0430\n",
      "iteration 9, loss 0.0412\n",
      "iteration 10, loss 0.0402\n",
      "iteration 11, loss 0.0376\n",
      "iteration 12, loss 0.0369\n",
      "iteration 13, loss 0.0347\n",
      "iteration 14, loss 0.0340\n",
      "iteration 15, loss 0.0331\n",
      "iteration 16, loss 0.0321\n",
      "iteration 17, loss 0.0317\n",
      "iteration 18, loss 0.0306\n",
      "iteration 19, loss 0.0302\n",
      "iteration 20, loss 0.0296\n",
      "iteration 21, loss 0.0295\n",
      "iteration 22, loss 0.0288\n",
      "iteration 23, loss 0.0287\n",
      "iteration 24, loss 0.0280\n",
      "iteration 25, loss 0.0281\n",
      "iteration 26, loss 0.0275\n",
      "iteration 27, loss 0.0281\n",
      "iteration 28, loss 0.0273\n",
      "iteration 29, loss 0.0277\n",
      "iteration 30, loss 0.0271\n",
      "iteration 31, loss 0.0270\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import torch\n",
    "\n",
    "DIM = 128\n",
    "ITERATIONS = 32\n",
    "X_MAX = 100\n",
    "ALPHA = 3/4\n",
    "GPU_ID = 2\n",
    "\n",
    "n_words = X_ij.shape[0]\n",
    "\n",
    "X = torch.from_numpy(X_ij.astype('float32'))\n",
    "w_main = torch.FloatTensor(n_words, DIM).uniform_(-0.5, 0.5)\n",
    "w_context = torch.FloatTensor(n_words, DIM).uniform_(-0.5, 0.5)\n",
    "b_main = torch.FloatTensor(n_words).uniform_(-0.5, 0.5)\n",
    "b_context = torch.FloatTensor(n_words).uniform_(-0.5, 0.5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    X = X.cuda(device=GPU_ID)\n",
    "    w_main = w_main.cuda(device=GPU_ID)\n",
    "    w_context = w_context.cuda(device=GPU_ID)\n",
    "    b_main = b_main.cuda(device=GPU_ID)\n",
    "    b_context = b_context.cuda(device=GPU_ID)\n",
    "\n",
    "X.requires_grad_(False)\n",
    "w_main.requires_grad_(True)\n",
    "w_context.requires_grad_(True)\n",
    "b_main.requires_grad_(True)\n",
    "b_context.requires_grad_(True)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam([w_main, w_context, b_main, b_context],\n",
    "                             lr=1e-3, weight_decay=1e-15)\n",
    "\n",
    "with torch.set_grad_enabled(True):\n",
    "    for iteration in range(ITERATIONS):\n",
    "        acc_loss = 0\n",
    "        for j in torch.randperm(n_words):\n",
    "            output = w_main @ w_context[j]\n",
    "            output += b_main\n",
    "            output += b_context[j]\n",
    "            \n",
    "            loss = criterion(output, X[:, j].log() + 1e-15)\n",
    "            \n",
    "            loss_weight = (X[:, j] / X_MAX) ** ALPHA\n",
    "            loss_weight[X[:, j] > X_MAX] = 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(loss_weight)\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_loss += float(loss.mean())\n",
    "        \n",
    "        print(f'iteration {iteration}, loss {acc_loss/n_words:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "son            0.012575\n",
       "wife           0.012556\n",
       "mom            0.012284\n",
       "daughter       0.011946\n",
       "college        0.011625\n",
       "dad            0.011515\n",
       "husband        0.011464\n",
       "sister         0.011447\n",
       "hospital       0.011309\n",
       "student        0.011260\n",
       "brother        0.011109\n",
       "sat            0.010950\n",
       "walked         0.010775\n",
       "grandmother    0.010656\n",
       "colleagues     0.010607\n",
       "boy            0.010593\n",
       "favorite       0.010455\n",
       "bed            0.010370\n",
       "University     0.010326\n",
       "meeting        0.010304\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def v(word):\n",
    "    i = vocabulary.index.get_loc(word)\n",
    "    return w_main[i].cpu()\n",
    "\n",
    "def analogy(target, top_k=20):\n",
    "    target /= target.norm()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        similarity = (w_main.cpu() @ target) / (w_main.cpu().norm() * target.norm())\n",
    "        similarity = pd.Series(dict(zip(vocabulary.keys(), similarity.numpy())))\n",
    "        similarity = similarity[vocabulary < 500]\n",
    "        similarity = similarity.sort_values(ascending=False)\n",
    "    \n",
    "    return similarity.sort_values(ascending=False)[:top_k]\n",
    "\n",
    "analogy(v('husband') - v('man') + v('woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oh             0.005673\n",
       "Get            0.005164\n",
       "Come           0.005076\n",
       "yeah           0.004982\n",
       "Man            0.004979\n",
       "Ah             0.004951\n",
       "title          0.004930\n",
       "Don            0.004890\n",
       "sister         0.004881\n",
       "son            0.004869\n",
       "threw          0.004859\n",
       "bag            0.004843\n",
       "mouth          0.004802\n",
       "neck           0.004801\n",
       "click          0.004799\n",
       "dad            0.004791\n",
       "finger         0.004739\n",
       "Audience       0.004720\n",
       "grandmother    0.004713\n",
       "mom            0.004695\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(v('heaven') - v('good') + v('bad'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization using PCA and t-SNE\n",
    "\n",
    "![](https://scontent-icn1-1.xx.fbcdn.net/v/t1.0-9/41425661_1809264752526756_3946431284045152256_n.jpg?_nc_cat=107&oh=e0b118959eaf0d6c7c97ce71b8c1136d&oe=5C20EDF1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PCA\n",
    "  * good for dimensionality reduction\n",
    "  * not always good for visualization\n",
    "  * weak against non-linear data\n",
    "* t-SNE\n",
    "  * good for visualization\n",
    "  * not so good for dimensionality reduction\n",
    "  * strong against non-linear data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FastText](https://fasttext.cc/): Library for efficient text classification and representation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "fasttext = torchtext.vocab.FastText(language='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.vectors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(vocabulary, embeddings, n_points):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    frequent = vocabulary[vocabulary < 2000].sort_values(ascending=False).index[:n_points]\n",
    "    indices = [vocabulary.index.get_loc(word) for word in frequent]\n",
    "    \n",
    "    pca = PCA(n_components=2, random_state=0)\n",
    "    with torch.no_grad():\n",
    "        results = pca.fit_transform(embeddings[indices])\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(n_points):\n",
    "        query = vocabulary.index[indices[i]]\n",
    "        x, y = results[i]\n",
    "        plt.scatter(x, y, label=query)\n",
    "        \n",
    "        # Prevent label overlapping by applying random offsets.\n",
    "        offset_x = np.random.randint(-35, 12) / 2000\n",
    "        offset_y = np.random.randint(-30, 15) / 2000\n",
    "        \n",
    "        plt.annotate(query, (x + offset_x, y + offset_y))\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "pca(vocabulary, w_main, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "    \n",
    "def tsne(vocabulary, embeddings, n_points):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    frequent = vocabulary[vocabulary < 2000].sort_values(ascending=False).index[:n_points]\n",
    "    indices = [vocabulary.index.get_loc(word) for word in frequent]\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    with torch.no_grad():\n",
    "        results = tsne.fit_transform(embeddings[indices])\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(n_points):\n",
    "        query = vocabulary.index[indices[i]]\n",
    "        x, y = results[i]\n",
    "        plt.scatter(x, y, label=query)\n",
    "        \n",
    "        # Prevent label overlapping by applying random offsets.\n",
    "        offset_x = np.random.randint(-35, 12) / 100\n",
    "        offset_y = np.random.randint(-30, 15) / 100\n",
    "        \n",
    "        plt.annotate(query, (x + offset_x, y + offset_y))\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "tsne(vocabulary, w_main, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pytorch-v0.4.1)",
   "language": "python",
   "name": "pytorch-v0.4.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
